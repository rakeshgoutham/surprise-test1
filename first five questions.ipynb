{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f2518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b84d4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vyv gvgv vu vyv\n",
      "{'vyv': 2, 'gvgv': 1, 'vu': 1}\n"
     ]
    }
   ],
   "source": [
    "p=input().split()\n",
    "ans=dict()\n",
    "for i in p:\n",
    "    if i in ans:\n",
    "        ans[i]+=1\n",
    "    else:\n",
    "        ans[i]=1\n",
    "print(ans)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ea20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gyg\n",
      "Its a palindrome \n"
     ]
    }
   ],
   "source": [
    "string =input()\n",
    "k=string[::-1]\n",
    "if(k==string):\n",
    "    print(\"Its a palindrome \")\n",
    "else:\n",
    "    print(\"Not a palindrome \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc0b11",
   "metadata": {},
   "source": [
    "# Steps in ML lifecycle\n",
    "\n",
    "1. UNDERSTANDING THE PROBLEM STATEMENT\n",
    "2. USAGE OF DIFFERENT ALGORITHMS TO DIFFERENT PROBLEM STATEMNTS \n",
    "3. DATA ANALYSIS\n",
    "4. DATA PREPROCESSING \n",
    "5. DATA VISUALIZATION \n",
    "6. EVALUATE THE MODEL\n",
    "7. TRAIN AND BUILD THE MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e2bab",
   "metadata": {},
   "source": [
    "# TYPES OF PREPROCESSING DONE :\n",
    "  \n",
    "  ## Data cleaning :\n",
    "   Data Cleaning/Cleansing routines attempt to fill in missing values, smooth out noise while identifying outliers, and correct inconsistencies in the data.\n",
    "  \n",
    "  ## Data Integration :\n",
    "  Data Integration is involved in data analysis task which combines data from multiple sources\n",
    "  \n",
    "  ## Data Transfromation :\n",
    "  1. Normalisation, where the attribute data are scaled to fall within a small specified range\n",
    "  2. Smoothing works to remove the noise from the data. Such techniques include binning, clustering, and regression.\n",
    "\n",
    " ## Data Reduction :\n",
    " In Dimension Reduction, irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48bbee",
   "metadata": {},
   "source": [
    "# UNDERFITTING AND OVERFITTING \n",
    "\n",
    "## UNDERFITTING :\n",
    "\n",
    " 1. It represents that our model or the algorithm does not fit to the data well \n",
    " 2. it happens when we dont have sufficient amount of data \n",
    " 3. It decreases the accuracy of the model\n",
    " 4. Underfitting – High bias and low variance \n",
    " ### Techniques to reduce underfitting: \n",
    "1. Increase model complexity \n",
    "2. Increase the number of features\n",
    "3. Remove noise from the data. \n",
    " \n",
    " ## OVERFITTING \n",
    "\n",
    "1. It contanins amount of data which is (more than sufficient) \n",
    " 2. Too much amount of data is given to the model is known as OVERFITTING \n",
    "  3. Overfitting – High variance and low bias \n",
    "  ### Techniques to reduce overfitting: \n",
    "1. Increase training data. \n",
    "2. Reduce model complexity. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7212202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
